import os
import asyncio
import logging
from logging.handlers import RotatingFileHandler
from pathlib import Path
from datetime import datetime

import pandas as pd
from watchfiles import awatch

from aiobotocore.session import get_session
from botocore.client import Config
from botocore.exceptions import ClientError


# =========================
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ env vars
# =========================
S3_ENDPOINT = os.getenv("S3_ENDPOINT", "https://s3.ru-3.storage.selcloud.ru")
S3_BUCKET = os.getenv("S3_BUCKET", "")
S3_PREFIX = os.getenv("S3_PREFIX", "task3/processed/")           # –∫—É–¥–∞ –≥—Ä—É–∑–∏–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
S3_LOG_KEY = os.getenv("S3_LOG_KEY", "task3/logs/pipeline.log")  # –∫—É–¥–∞ –≥—Ä—É–∑–∏–º –ª–æ–≥ (–æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ key)
S3_VERIFY_SSL = os.getenv("S3_VERIFY_SSL", "0").strip() not in ("0", "false", "False", "")

WATCH_DIR = Path(os.getenv("WATCH_DIR", "./inbox")).resolve()
ARCHIVE_DIR = Path(os.getenv("ARCHIVE_DIR", "./archive")).resolve()
TMP_DIR = Path(os.getenv("TMP_DIR", "./tmp")).resolve()
LOG_DIR = Path(os.getenv("LOG_DIR", "./logs")).resolve()

ACCESS_KEY = os.getenv("S3_ACCESS_KEY")
SECRET_KEY = os.getenv("S3_SECRET_KEY")


# =========================
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# =========================
def setup_logger() -> logging.Logger:
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    log_path = LOG_DIR / "pipeline.log"

    logger = logging.getLogger("task3_pipeline")
    logger.setLevel(logging.INFO)

    # —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å —Ö—ç–Ω–¥–ª–µ—Ä—ã –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—Ö –≤ IDE
    if logger.handlers:
        return logger

    handler = RotatingFileHandler(
        log_path, maxBytes=2_000_000, backupCount=3, encoding="utf-8"
    )
    fmt = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
    handler.setFormatter(fmt)
    logger.addHandler(handler)

    console = logging.StreamHandler()
    console.setFormatter(fmt)
    logger.addHandler(console)

    return logger


LOGGER = setup_logger()


# =========================
# S3 Async Client (Selectel)
# =========================
class AsyncS3:
    def __init__(self):
        if not ACCESS_KEY or not SECRET_KEY or not S3_BUCKET:
            raise RuntimeError(
                "–ù–µ –∑–∞–¥–∞–Ω—ã S3_ACCESS_KEY / S3_SECRET_KEY / S3_BUCKET –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è."
            )

        self._session = get_session()
        self._auth = {
            "aws_access_key_id": ACCESS_KEY,
            "aws_secret_access_key": SECRET_KEY,
            "endpoint_url": S3_ENDPOINT,
            "region_name": "us-east-1",
            "config": Config(signature_version="s3v4"),
            "verify": S3_VERIFY_SSL,  # False, –µ—Å–ª–∏ —É —Ç–µ–±—è self-signed —Ü–µ–ø–æ—á–∫–∞
        }

    async def put_file(self, local_path: Path, key: str):
        async with self._session.create_client("s3", **self._auth) as s3:
            with local_path.open("rb") as f:
                await s3.put_object(Bucket=S3_BUCKET, Key=key, Body=f)

    async def put_bytes(self, data: bytes, key: str):
        async with self._session.create_client("s3", **self._auth) as s3:
            await s3.put_object(Bucket=S3_BUCKET, Key=key, Body=data)


# =========================
# –£—Ç–∏–ª–∏—Ç—ã
# =========================
async def wait_file_stable(path: Path, checks: int = 5, delay: float = 0.4) -> None:
    """
    –ñ–¥—ë–º, –ø–æ–∫–∞ —Ñ–∞–π–ª –ø–µ—Ä–µ—Å—Ç–∞–Ω–µ—Ç –º–µ–Ω—è—Ç—å—Å—è –ø–æ —Ä–∞–∑–º–µ—Ä—É.
    –≠—Ç–æ –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç —Å–∏—Ç—É–∞—Ü–∏–∏, –∫–æ–≥–¥–∞ —Ñ–∞–π–ª –µ—â—ë –¥–æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è.
    """
    last = -1
    stable_count = 0

    for _ in range(checks * 5):
        if not path.exists():
            return
        size = path.stat().st_size
        if size == last and size > 0:
            stable_count += 1
            if stable_count >= checks:
                return
        else:
            stable_count = 0
            last = size
        await asyncio.sleep(delay)


def process_csv(input_path: Path, tmp_dir: Path) -> Path:
    """
    –ß–∏—Ç–∞–µ–º CSV –≤ pandas, –¥–µ–ª–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π CSV –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É.
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–ø—Ä–∏–º–µ—Ä):
      - –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ 'value' (—á–∏—Å–ª–æ–≤–∞—è) -> –±–µ—Ä—ë–º value > 50
      - –∏–Ω–∞—á–µ –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –ø—É—Å—Ç–æ—Ç –≤ –ø–µ—Ä–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ
    """
    df = pd.read_csv(input_path)

    if "value" in df.columns:
        # —Å—Ç–∞—Ä–∞–µ–º—Å—è –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª—É, –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–∞–Ω—É—Ç NaN
        df["value"] = pd.to_numeric(df["value"], errors="coerce")
        out = df[df["value"] > 50].copy()
    else:
        first_col = df.columns[0]
        out = df[df[first_col].notna()].copy()

    tmp_dir.mkdir(parents=True, exist_ok=True)
    out_name = input_path.stem + "_processed.csv"
    out_path = tmp_dir / out_name
    out.to_csv(out_path, index=False)
    return out_path


def archive_file(src: Path, archive_dir: Path) -> Path:
    """
    –ü–µ—Ä–µ–º–µ—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –≤ archive/ —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º timestamp, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ç–∏—Ä–∞—Ç—å.
    """
    archive_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    dst = archive_dir / f"{src.stem}__arch_{ts}{src.suffix}"
    src.rename(dst)
    return dst


async def upload_log_to_s3(s3: AsyncS3, log_path: Path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–≥ –≤ S3 –æ–¥–Ω–∏–º –∏ —Ç–µ–º –∂–µ –∫–ª—é—á–æ–º.
    –ï—Å–ª–∏ –≤ –±–∞–∫–µ—Ç–µ –≤–∫–ª—é—á–µ–Ω–æ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Äî –ø–æ—è–≤—è—Ç—Å—è –≤–µ—Ä—Å–∏–∏ –ª–æ–≥–∞.
    """
    if not log_path.exists():
        return
    await s3.put_file(log_path, S3_LOG_KEY)


# =========================
# –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
# =========================
async def handle_file(s3: AsyncS3, file_path: Path):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è -> pandas -> tmp -> upload -> archive -> upload log.
    """
    try:
        LOGGER.info(f"üîé –ù–æ–≤—ã–π —Ñ–∞–π–ª –æ–±–Ω–∞—Ä—É–∂–µ–Ω: {file_path.name}")
        await wait_file_stable(file_path)

        if not file_path.exists():
            LOGGER.warning(f"–§–∞–π–ª –∏—Å—á–µ–∑ –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {file_path}")
            return

        if file_path.suffix.lower() != ".csv":
            LOGGER.info(f"–ü—Ä–æ–ø—É—Å–∫–∞—é (–Ω–µ CSV): {file_path.name}")
            return

        LOGGER.info("üì• –ß–∏—Ç–∞—é –∏ —Ñ–∏–ª—å—Ç—Ä—É—é —á–µ—Ä–µ–∑ pandas...")
        processed_path = process_csv(file_path, TMP_DIR)
        LOGGER.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ñ–∞–π–ª: {processed_path.name}")

        # –∫–ª—é—á –≤ S3: –ø—Ä–µ—Ñ–∏–∫—Å + –∏–º—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        s3_key = f"{S3_PREFIX}{processed_path.name}"
        LOGGER.info(f"‚òÅÔ∏è –ó–∞–≥—Ä—É–∂–∞—é –≤ S3: s3://{S3_BUCKET}/{s3_key}")
        await s3.put_file(processed_path, s3_key)
        LOGGER.info("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3 —É—Å–ø–µ—à–Ω–∞")

        archived = archive_file(file_path, ARCHIVE_DIR)
        LOGGER.info(f"üì¶ –ò—Å—Ö–æ–¥–Ω–∏–∫ –ø–µ—Ä–µ–º–µ—â—ë–Ω –≤ –∞—Ä—Ö–∏–≤: {archived.name}")

        # –ü–µ—Ä–µ–∑–∞–ª–∏–≤–∞–µ–º –ª–æ–≥ –≤ S3 (–¥–ª—è –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ª–æ–≥–∞)
        log_path = (LOG_DIR / "pipeline.log")
        LOGGER.info(f"üßæ –ó–∞–≥—Ä—É–∂–∞—é –ª–æ–≥ –≤ S3: s3://{S3_BUCKET}/{S3_LOG_KEY}")
        await upload_log_to_s3(s3, log_path)
        LOGGER.info("‚úÖ –õ–æ–≥ –∑–∞–≥—Ä—É–∂–µ–Ω –≤ S3")

    except ClientError as e:
        code = e.response.get("Error", {}).get("Code", "")
        msg = e.response.get("Error", {}).get("Message", str(e))
        LOGGER.error(f"‚ùå S3 ClientError: {code} ‚Äî {msg}")
    except Exception as e:
        LOGGER.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")


async def main():
    WATCH_DIR.mkdir(parents=True, exist_ok=True)
    ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
    TMP_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)

    LOGGER.info("=== Task 3 pipeline started ===")
    LOGGER.info(f"Watching folder: {WATCH_DIR}")
    LOGGER.info(f"S3 endpoint: {S3_ENDPOINT}")
    LOGGER.info(f"S3 bucket:   {S3_BUCKET}")
    LOGGER.info(f"S3 prefix:   {S3_PREFIX}")
    LOGGER.info(f"SSL verify:  {S3_VERIFY_SSL}")

    s3 = AsyncS3()

    # –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π watcher
    async for changes in awatch(WATCH_DIR):
        # changes: set of (Change, path)
        for _, changed_path in changes:
            p = Path(changed_path)
            if p.is_file():
                # –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Ñ–æ–Ω–µ (–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ)
                asyncio.create_task(handle_file(s3, p))


if __name__ == "__main__":
    asyncio.run(main())
